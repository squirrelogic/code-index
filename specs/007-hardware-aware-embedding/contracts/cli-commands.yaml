# CLI Commands Contract: Hardware-Aware Embedding

# This contract defines the CLI interface for embedding-related commands.
# Format follows OpenAPI-style specification for CLI tools.

version: '1.0.0'
commands:

  # ============================================================================
  # config set embedding.*
  # ============================================================================

  - name: config set embedding.profile
    description: Set the active embedding profile
    usage: code-index config set embedding.profile <profile>

    arguments:
      - name: profile
        type: string
        required: true
        description: Profile name (light, balanced, performance, or custom profile name)
        enum: [light, balanced, performance]
        allowCustom: true

    options: []

    examples:
      - command: code-index config set embedding.profile light
        description: Switch to light profile (CPU-optimized)

      - command: code-index config set embedding.profile performance
        description: Switch to performance profile (GPU-accelerated)

    behavior:
      - Updates .codeindex/config.json with new profile settings
      - Validates profile exists (preset or custom)
      - Warns if profile incompatible with hardware
      - Takes effect on next embedding operation (no restart required)
      - If dimension changes, warns about cache invalidation

    exitCodes:
      0: Success - profile updated
      1: Invalid profile name
      2: Configuration file not writable
      3: Hardware incompatible with profile

    output:
      success: |
        ✓ Profile updated to 'light'
        Configuration saved to .codeindex/config.json
        Changes will take effect on next embedding operation

      dimensionChange: |
        ⚠ Dimension change detected: 768 → 384
        This will invalidate the embedding cache
        Run 'code-index embed --rebuild' to regenerate embeddings

      hardwareIncompatible: |
        ⚠ Warning: Performance profile requires GPU
        Detected hardware: CPU-only
        Profile will auto-fallback to CPU when used

  # ============================================================================

  - name: config set embedding.model
    description: Set the embedding model
    usage: code-index config set embedding.model <model-id>

    arguments:
      - name: model-id
        type: string
        required: true
        description: Hugging Face model ID (e.g., Xenova/all-MiniLM-L6-v2) or local path

    options:
      - name: --version
        short: -v
        type: string
        description: Specific model version/commit hash (default: main)
        default: main

    examples:
      - command: code-index config set embedding.model Xenova/all-MiniLM-L6-v2
        description: Use MiniLM model from Hugging Face

      - command: code-index config set embedding.model Xenova/all-mpnet-base-v2 --version v1.0.0
        description: Use specific version of mpnet model

      - command: code-index config set embedding.model /path/to/local/model
        description: Use local model directory

    behavior:
      - Updates model configuration in .codeindex/config.json
      - Validates model exists (HF Hub or local path)
      - Detects model dimensions automatically
      - If dimensions differ from current, warns about cache invalidation
      - Model will be downloaded on next embedding operation if not cached

    exitCodes:
      0: Success - model configuration updated
      1: Invalid model ID or path not found
      2: Configuration file not writable
      3: Model dimensions incompatible
      4: Network error (if validating remote model)

    output:
      success: |
        ✓ Model updated to 'Xenova/all-mpnet-base-v2'
        Dimensions: 768
        Model will be downloaded on first use (~200MB)

      dimensionChange: |
        ⚠ Dimension change detected: 384 → 768
        Embedding cache will be invalidated
        Run 'code-index embed --rebuild' to regenerate

  # ============================================================================

  - name: config set embedding.backend
    description: Set the embedding backend
    usage: code-index config set embedding.backend <backend>

    arguments:
      - name: backend
        type: string
        required: true
        description: Backend to use for embeddings
        enum: [onnx, pytorch]

    options: []

    examples:
      - command: code-index config set embedding.backend onnx
        description: Use ONNX Runtime (recommended)

    behavior:
      - Updates backend in .codeindex/config.json
      - Validates backend is available
      - ONNX is recommended for production use
      - Takes effect on next embedding operation

    exitCodes:
      0: Success - backend updated
      1: Invalid backend value
      2: Backend not available
      3: Configuration file not writable

    output:
      success: |
        ✓ Backend updated to 'onnx'
        Configuration saved

  # ============================================================================

  - name: config set embedding.quantization
    description: Set the quantization level
    usage: code-index config set embedding.quantization <level>

    arguments:
      - name: level
        type: string
        required: true
        description: Quantization precision level
        enum: [int8, int4, fp16, fp32]

    options: []

    examples:
      - command: code-index config set embedding.quantization int8
        description: Use int8 quantization (faster, less memory)

      - command: code-index config set embedding.quantization fp16
        description: Use fp16 half precision (GPU recommended)

    behavior:
      - Updates quantization in .codeindex/config.json
      - Validates quantization supported by backend + device
      - int8/int4: CPU-optimized, 2x faster
      - fp16: GPU-optimized, minimal quality loss
      - fp32: Highest quality, slowest

    exitCodes:
      0: Success - quantization updated
      1: Invalid quantization value
      2: Quantization not supported by backend/device
      3: Configuration file not writable

    output:
      success: |
        ✓ Quantization updated to 'int8'
        Expected speedup: 2x
        Expected quality loss: <1%

  # ============================================================================

  - name: config set embedding.batchSize
    description: Set the batch size for processing
    usage: code-index config set embedding.batchSize <size>

    arguments:
      - name: size
        type: integer
        required: true
        description: Number of embeddings to process simultaneously
        constraints:
          min: 1
          max: 256

    options: []

    examples:
      - command: code-index config set embedding.batchSize 32
        description: Process 32 embeddings per batch

    behavior:
      - Updates batchSize in .codeindex/config.json
      - Must be between 1-256
      - Higher values = faster but more memory
      - System may auto-reduce if OOM occurs
      - Recommended: 16 (CPU), 32 (MPS), 64 (CUDA)

    exitCodes:
      0: Success - batch size updated
      1: Invalid value (out of range)
      2: Configuration file not writable

    output:
      success: |
        ✓ Batch size updated to 32
        Configuration saved

  # ============================================================================

  - name: config set embedding.cacheDir
    description: Set the embedding cache directory
    usage: code-index config set embedding.cacheDir <path>

    arguments:
      - name: path
        type: string
        required: true
        description: Directory path for embedding cache (relative or absolute)

    options: []

    examples:
      - command: code-index config set embedding.cacheDir .codeindex/cache
        description: Use default cache location

      - command: code-index config set embedding.cacheDir /tmp/embeddings
        description: Use custom cache location

    behavior:
      - Updates cacheDir in .codeindex/config.json
      - Creates directory if it doesn't exist
      - Validates directory is writable
      - Existing cache remains at old location (not moved)

    exitCodes:
      0: Success - cache directory updated
      1: Invalid path
      2: Directory not writable
      3: Configuration file not writable

    output:
      success: |
        ✓ Cache directory updated to '/tmp/embeddings'
        Directory created
        Existing cache not moved (use --move to relocate)

  # ============================================================================
  # embed
  # ============================================================================

  - name: embed
    description: Generate embeddings for indexed files
    usage: code-index embed [options]

    arguments: []

    options:
      - name: --rebuild
        short: -r
        type: boolean
        description: Force rebuild of all embeddings (ignores cache)
        default: false

      - name: --files
        short: -f
        type: string[]
        description: Specific files to embed (default: all indexed files)
        default: []

      - name: --profile
        short: -p
        type: string
        description: Override profile for this run (doesn't save to config)
        enum: [light, balanced, performance]

      - name: --progress
        type: boolean
        description: Show progress bar
        default: true

      - name: --json
        type: boolean
        description: Output results as JSON
        default: false

    examples:
      - command: code-index embed
        description: Generate embeddings for all indexed files (uses cache)

      - command: code-index embed --rebuild
        description: Regenerate all embeddings from scratch

      - command: code-index embed --files src/main.ts src/utils.ts
        description: Embed specific files only

      - command: code-index embed --profile light
        description: Use light profile for this run

      - command: code-index embed --json
        description: Output results as JSON

    behavior:
      - Loads configuration from .codeindex/config.json
      - Detects hardware if not already cached
      - Downloads model if not cached locally
      - Processes files in batches based on config.batchSize
      - Caches embeddings to .codeindex/cache/embeddings.db
      - Shows progress bar unless --json or --no-progress
      - Implements fallback chain on errors
      - Returns non-zero exit code on failure

    exitCodes:
      0: Success - all embeddings generated
      1: Configuration error
      2: Model load failure
      3: Hardware detection failure
      4: Partial failure (some files failed, see --json output)
      5: Complete failure (all fallbacks exhausted)

    output:
      success: |
        ✓ Hardware detected: NVIDIA GeForce RTX 3090 (24GB)
        ✓ Model loaded: Xenova/all-mpnet-base-v2 (768 dimensions)
        ⠋ Generating embeddings...
        [████████████████████] 100% | 1000/1000 files | ETA: 0s | Mem: 420MB
        ✓ Embeddings generated successfully

        Summary:
          Total: 1000 files
          Cached: 850 files (hits)
          Generated: 150 files (new/updated)
          Failed: 0 files
          Duration: 15.2s
          Throughput: 65.8 files/sec

      fallback: |
        ⚠ CUDA out of memory (batch size 64)
        → Fallback: Reducing batch size to 32
        ✓ Continued successfully with smaller batch size

      json: |
        {
          "success": true,
          "summary": {
            "total": 1000,
            "cached": 850,
            "generated": 150,
            "failed": 0,
            "duration": 15.2,
            "throughput": 65.8
          },
          "hardware": {
            "device": "cuda",
            "gpu": "NVIDIA GeForce RTX 3090",
            "memory": 25769803776
          },
          "model": {
            "id": "Xenova/all-mpnet-base-v2",
            "dimensions": 768,
            "quantization": "fp16"
          },
          "fallbacks": [],
          "failures": []
        }

  # ============================================================================
  # doctor (embedding diagnostics)
  # ============================================================================

  - name: doctor
    description: Display system diagnostics and embedding configuration
    usage: code-index doctor [options]

    arguments: []

    options:
      - name: --json
        type: boolean
        description: Output diagnostics as JSON
        default: false

      - name: --verbose
        short: -v
        type: boolean
        description: Show detailed information
        default: false

    examples:
      - command: code-index doctor
        description: Show system diagnostics

      - command: code-index doctor --json
        description: Output diagnostics as JSON

      - command: code-index doctor --verbose
        description: Show detailed hardware and model information

    behavior:
      - Re-detects hardware capabilities
      - Loads current configuration
      - Validates model availability
      - Checks ONNX Runtime providers
      - Reports any configuration issues
      - Shows recent fallback history
      - Provides recommendations

    exitCodes:
      0: Success - system healthy
      1: Configuration issues detected
      2: Hardware detection failed
      3: Model validation failed

    output:
      success: |
        Code-Index Embedding Diagnostics
        ================================

        Hardware:
          CPU: Apple M1 Pro (10 cores)
          RAM: 32.0 GB total, 8.0 GB free
          GPU: Apple M1 Pro (16 GB unified memory)
          Platform: macOS (arm64)

        Configuration:
          Profile: balanced
          Model: Xenova/all-mpnet-base-v2
          Dimensions: 768
          Backend: onnx
          Device: mps
          Quantization: fp16
          Batch Size: 32

        Model Status:
          ✓ Model cached locally (~200 MB)
          ✓ Model compatible with hardware
          Path: .codeindex/models/Xenova--all-mpnet-base-v2

        ONNX Runtime:
          Available Providers: [CoreMLExecutionProvider, CPUExecutionProvider]
          Active Provider: CoreMLExecutionProvider

        Cache:
          Location: .codeindex/cache/embeddings.db
          Size: 3.2 MB
          Entries: 1,000 embeddings
          Hit Rate: 85% (last 1000 operations)

        Recent Fallbacks: None

        Recommendations:
          ✓ System configuration optimal for detected hardware
          ✓ No issues detected

      issuesDetected: |
        Code-Index Embedding Diagnostics
        ================================

        ⚠ Issues Detected:

        1. CUDA device configured but not available
           Current: device = cuda
           Detected: No NVIDIA GPU found
           Fix: code-index config set embedding.device cpu

        2. Batch size too large for available memory
           Current: batchSize = 256
           Available RAM: 2.0 GB free
           Recommendation: code-index config set embedding.batchSize 32

        3. Model not cached locally
           Model: Xenova/instructor-large (~500 MB)
           First run will download model (requires network)

      json: |
        {
          "healthy": true,
          "hardware": {
            "cpu": { "model": "Apple M1 Pro", "cores": 10 },
            "ram": { "total": 34359738368, "free": 8589934592 },
            "gpu": {
              "vendor": "Apple",
              "name": "Apple M1 Pro",
              "memory": 17179869184
            },
            "platform": "darwin",
            "arch": "arm64"
          },
          "configuration": {
            "profile": "balanced",
            "model": "Xenova/all-mpnet-base-v2",
            "dimensions": 768,
            "backend": "onnx",
            "device": "mps",
            "quantization": "fp16",
            "batchSize": 32
          },
          "modelStatus": {
            "cached": true,
            "size": 209715200,
            "path": ".codeindex/models/Xenova--all-mpnet-base-v2",
            "compatible": true
          },
          "onnxRuntime": {
            "availableProviders": ["CoreMLExecutionProvider", "CPUExecutionProvider"],
            "activeProvider": "CoreMLExecutionProvider"
          },
          "cache": {
            "location": ".codeindex/cache/embeddings.db",
            "size": 3355443,
            "entries": 1000,
            "hitRate": 0.85
          },
          "recentFallbacks": [],
          "issues": [],
          "recommendations": [
            "System configuration optimal for detected hardware"
          ]
        }

# ============================================================================
# Shared Types
# ============================================================================

types:
  Profile:
    type: string
    enum: [light, balanced, performance]
    description: Preset embedding profile optimized for different hardware

  ModelID:
    type: string
    pattern: '^[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+$|^(/|\./).*$'
    description: Hugging Face model ID (org/model) or local path

  Backend:
    type: string
    enum: [onnx, pytorch]
    description: Embedding inference backend

  Device:
    type: string
    enum: [cpu, mps, cuda, auto]
    description: Hardware device for inference

  Quantization:
    type: string
    enum: [int8, int4, fp16, fp32]
    description: Model quantization level

# ============================================================================
# Global Options (apply to all commands)
# ============================================================================

globalOptions:
  - name: --help
    short: -h
    description: Show help for command

  - name: --version
    short: -V
    description: Show version number

  - name: --cwd
    type: string
    description: Set working directory
    default: process.cwd()

  - name: --verbose
    short: -v
    description: Enable verbose logging
    default: false

  - name: --quiet
    short: -q
    description: Suppress non-error output
    default: false

# ============================================================================
# Error Handling
# ============================================================================

errorHandling:
  format: |
    ✗ Error: <message>

    <details if available>

    For help, run: code-index <command> --help

  jsonFormat: |
    {
      "success": false,
      "error": {
        "code": "<error-code>",
        "message": "<message>",
        "details": "<details>"
      }
    }

# ============================================================================
# Performance Requirements
# ============================================================================

performance:
  - command: config set
    requirement: Complete within 100ms

  - command: embed
    requirement: |
      - Hardware detection: <2s
      - Model loading: 2-15s (first time)
      - Throughput: 50+ files/sec (CPU), 100+ files/sec (GPU)
      - Memory: <500MB baseline, up to 80% available with auto-adjustment

  - command: doctor
    requirement: Complete within 2s (includes hardware re-detection)
