# CLI Contract: code-index embed Command

version: 1.0.0
command: embed
description: Generate or update embedding vectors for code chunks in the index

## Command Signature

```bash
code-index embed [options]
```

## Options

### --model <name>
- **Type**: string
- **Required**: No
- **Default**: "all-MiniLM-L6-v2" (from model_configurations where is_default = 1)
- **Description**: Override the default embedding model
- **Examples**:
  - `--model all-MiniLM-L6-v2`
  - `--model openai-text-embedding-3-small`
- **Validation**: Model must exist in available adapters registry

### --dry-run
- **Type**: boolean flag
- **Required**: No
- **Default**: false
- **Description**: Preview what would be embedded without performing the operation
- **Examples**: `--dry-run`
- **Behavior**: Reports statistics without modifying the database

### --force
- **Type**: boolean flag
- **Required**: No
- **Default**: false
- **Description**: Force re-embedding of all chunks, even if unchanged
- **Examples**: `--force`
- **Warning**: May take significant time for large codebases

### --batch-size <number>
- **Type**: integer
- **Required**: No
- **Default**: 16
- **Range**: 1-100
- **Description**: Number of chunks to process in each batch
- **Examples**: `--batch-size 32`
- **Performance Impact**: Higher values improve throughput but increase memory usage

### --json
- **Type**: boolean flag
- **Required**: No
- **Default**: false
- **Description**: Output results in JSON format for scripting
- **Examples**: `--json`

### --verbose, -v
- **Type**: boolean flag
- **Required**: No
- **Default**: false
- **Description**: Enable detailed progress output
- **Examples**: `-v` or `--verbose`

### --quiet, -q
- **Type**: boolean flag
- **Required**: No
- **Default**: false
- **Description**: Suppress all output except errors
- **Examples**: `-q` or `--quiet`
- **Conflicts**: Cannot be used with `--verbose`

## Exit Codes

- **0**: Success - all embeddings generated successfully
- **1**: General error - operation failed
- **2**: Invalid arguments - command line options validation failed
- **3**: Model not found - specified model does not exist
- **4**: Database error - failed to read or write to index
- **5**: Adapter error - embedding generation failed
- **6**: Interrupted - user cancelled operation (SIGINT)

## Output Formats

### Standard Output (Human-Readable)

#### Success Case

```
Embedding codebase chunks...

Model: all-MiniLM-L6-v2 (384 dimensions)
Strategy: Incremental (hash-based change detection)

Analyzing chunks...
  Total chunks: 1,234
  Already embedded: 1,150
  Needs embedding: 84
  Unchanged: 1,150

Generating embeddings...
[██████████████████████████████] 100% (84/84)

Summary:
  Embedded: 84 chunks
  Skipped: 1,150 chunks (no changes)
  Deleted: 0 chunks
  Duration: 12.3s
  Throughput: 6.8 chunks/sec

✓ Embedding complete
```

#### Dry-Run Output

```
Dry-run mode: No changes will be made

Model: all-MiniLM-L6-v2 (384 dimensions)

Analysis:
  Total chunks: 1,234
  Would embed: 84 chunks
  Would skip: 1,150 chunks (no changes)
  Would delete: 0 chunks

Estimated duration: ~12s
Estimated throughput: ~7 chunks/sec

Run without --dry-run to perform embedding.
```

#### Error Output

```
Error: Failed to generate embeddings

Model: all-MiniLM-L6-v2
Error: Adapter initialization failed: Model file not found at .codeindex/models/all-MiniLM-L6-v2.onnx

Suggestion: Run 'code-index doctor' to verify installation
            or download model with 'code-index download-model all-MiniLM-L6-v2'

Exit code: 5
```

### JSON Output Format

```json
{
  "command": "embed",
  "status": "success",
  "model": {
    "id": "all-MiniLM-L6-v2",
    "dimensions": 384,
    "adapter_type": "onnx"
  },
  "strategy": "incremental",
  "stats": {
    "total_chunks": 1234,
    "embedded": 84,
    "skipped": 1150,
    "deleted": 0,
    "duration_ms": 12345,
    "throughput_per_sec": 6.8
  },
  "started_at": "2025-10-14T12:34:56.789Z",
  "completed_at": "2025-10-14T12:35:09.034Z"
}
```

#### JSON Error Format

```json
{
  "command": "embed",
  "status": "error",
  "error": {
    "code": "ADAPTER_INIT_FAILED",
    "message": "Model file not found at .codeindex/models/all-MiniLM-L6-v2.onnx",
    "suggestion": "Run 'code-index doctor' to verify installation"
  },
  "exit_code": 5
}
```

## Behavioral Requirements

### FR-004: Incremental Embedding
- **Default behavior**: Only embed chunks where `chunk_hash` differs from stored `chunk_hash`
- **Override**: Use `--force` to re-embed all chunks regardless of hash

### FR-006: Dry-Run Mode
- **Required**: Must report accurate statistics without modifying database
- **Output**: Display what would be embedded, skipped, and deleted
- **Performance**: Must complete in < 5 seconds for codebases up to 10,000 files (SC-004)

### FR-011: Clear Error Messages
- **Required**: All errors must include:
  - What failed (e.g., "Adapter initialization failed")
  - Why it failed (e.g., "Model file not found")
  - How to fix it (e.g., "Run 'code-index doctor'")

### FR-015: Progress Reporting
- **Required**: Display progress for long-running operations
- **Format**: Progress bar with percentage, current/total, and throughput
- **Update frequency**: At least every 500ms

### FR-020: Model Switching
- **Behavior**: When switching to a model with different dimensions:
  1. Detect dimension mismatch
  2. Warn user that all embeddings will be cleared
  3. Require confirmation (unless `--force` is used)
  4. Delete all existing embeddings
  5. Re-embed all chunks with new model

## Examples

### Basic Usage

```bash
# Embed using default model
code-index embed

# Embed with specific model
code-index embed --model all-MiniLM-L6-v2

# Preview changes without embedding
code-index embed --dry-run

# Force re-embed all chunks
code-index embed --force

# Verbose output
code-index embed --verbose
```

### Advanced Usage

```bash
# Use hosted model with custom batch size
code-index embed --model openai-text-embedding-3-small --batch-size 32

# JSON output for scripting
code-index embed --json | jq '.stats.embedded'

# Quiet mode (only errors)
code-index embed --quiet

# Dry-run with JSON output
code-index embed --dry-run --json
```

## Dependencies

### Prerequisites
- **Database**: `.codeindex/index.db` must exist (created by `code-index init`)
- **Index**: Chunks table must be populated (created by `code-index index`)
- **Model**: Default model must be configured in `model_configurations` table

### Database Tables
- **Read**: `chunks` (to get code chunks and hashes)
- **Read**: `model_configurations` (to get model settings)
- **Write**: `vec_embeddings` (to store embedding vectors)
- **Write**: `embedding_metadata` (to store processing metadata)
- **Write**: `embedding_operations_log` (to log operation)

## Performance Expectations

From Success Criteria:

- **SC-001**: Incremental updates 10x faster than full re-index for <5% changes
- **SC-002**: Process at least 100 chunks/second for local models
- **SC-004**: Dry-run completes in < 5 seconds for codebases up to 10,000 files

### Expected Timings

| Codebase Size | Chunks | Full Embed | Incremental (5% change) |
|---------------|--------|------------|-------------------------|
| Small | 1,000 | ~10s | ~1s |
| Medium | 10,000 | ~100s | ~10s |
| Large | 100,000 | ~1000s | ~100s |

## Testing Contract

### Contract Tests

```typescript
describe('embed command contract', () => {
  it('should exit 0 on success', async () => {
    const result = await exec('code-index embed');
    expect(result.exitCode).toBe(0);
  });

  it('should exit 2 for invalid arguments', async () => {
    const result = await exec('code-index embed --invalid-flag');
    expect(result.exitCode).toBe(2);
  });

  it('should output valid JSON with --json flag', async () => {
    const result = await exec('code-index embed --json');
    const json = JSON.parse(result.stdout);
    expect(json).toHaveProperty('status');
    expect(json).toHaveProperty('stats');
  });

  it('should respect --dry-run flag', async () => {
    const before = await countEmbeddings();
    await exec('code-index embed --dry-run');
    const after = await countEmbeddings();
    expect(after).toBe(before); // No changes
  });

  it('should display progress with --verbose', async () => {
    const result = await exec('code-index embed --verbose');
    expect(result.stdout).toContain('%'); // Progress percentage
  });
});
```

## Validation Rules

### Input Validation

1. **Model Name**:
   - Must not be empty
   - Must exist in adapter registry
   - Pattern: `^[a-zA-Z0-9\-\.]+$`

2. **Batch Size**:
   - Must be integer
   - Range: 1 <= batch_size <= 100
   - Default: 16

3. **Flag Conflicts**:
   - `--verbose` and `--quiet` cannot be used together
   - `--dry-run` and `--force` can be used together (shows what --force would do)

### State Validation

1. **Database Exists**:
   - Must have `.codeindex/index.db`
   - Must have valid schema with required tables

2. **Chunks Indexed**:
   - Must have at least one chunk in `chunks` table
   - Error if index is empty: "No chunks found. Run 'code-index index' first."

3. **Model Configuration**:
   - Must have at least one model in `model_configurations`
   - Must have exactly one default model (is_default = 1)

## Idempotency

Per Constitution Principle II, the embed command MUST be idempotent:

- Running `code-index embed` multiple times produces the same result
- If no chunks changed, subsequent runs skip all chunks (no-op)
- If some chunks changed, only those chunks are re-embedded
- Safe to run in automated scripts without side effects

## Security Considerations

### Environment Variables

Hosted adapters may require API keys:

```bash
# Example: OpenAI adapter
export EMBED_OPENAI_API_KEY=sk-...
code-index embed --model openai-text-embedding-3-small
```

### Validation

- Never log full API keys (show only last 4 characters)
- Validate API keys before starting expensive operations
- Handle authentication errors gracefully with clear messages

## Changelog

- **1.0.0** (2025-10-14): Initial contract definition
